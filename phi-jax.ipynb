{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import load\n",
    "from flax import linen as nn\n",
    "from flax import traverse_util\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from jax import Array, pmap, random\n",
    "from transformers import AutoTokenizer\n",
    "from flax.training.train_state import TrainState\n",
    "from jax import numpy as jnp, ensure_compile_time_eval\n",
    "from flax.jax_utils import replicate, prefetch_to_device\n",
    "from optax import adamw, set_to_zero, multi_transform, l2_loss\n",
    "from jax import numpy as jnp, value_and_grad, ensure_compile_time_eval\n",
    "\n",
    "import jax.tools.colab_tpu\n",
    "jax.tools.colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.process_count(), jax.device_count(), jax.local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "\n",
    "def data_iter(n, batch_size: int):\n",
    "    dataset_split = dataset[\"train_sft\"]\n",
    "    local_device_count = jax.local_device_count()\n",
    "\n",
    "    i = 0\n",
    "    for _ in range(len(dataset_split)[:n] // batch_size):\n",
    "        batch = jnp.array([tokenizer.encode(x, return_tensors=\"np\", padding=\"max_length\", max_length=2048) for x in dataset_split[i:i+batch_size]])\n",
    "        batch = jax.tree_util.tree_map(lambda x: x.reshape((local_device_count, -1) + x.shape[1:]), batch)\n",
    "        batch = prefetch_to_device(batch, 2)\n",
    "        i += batch_size\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PhiConfig:\n",
    "    n_head: int = 32\n",
    "    n_layer: int = 24\n",
    "    n_embed: int = 2048\n",
    "    rotary_dim: int = 32\n",
    "    ln_eps: float = 1e-05\n",
    "    n_positions: int = 2048\n",
    "    vocab_size: int = 51200\n",
    "    param_dtype: jnp.dtype = jnp.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cos_sin(config: PhiConfig) -> (Array, Array):\n",
    "    t = jnp.arange(config.n_positions, dtype=jnp.float32)\n",
    "    inv_freq = 1 / (10000 ** (jnp.arange(0, config.rotary_dim, 2, dtype=jnp.float32) / config.rotary_dim))\n",
    "    freqs = jnp.outer(t, inv_freq)\n",
    "    return jnp.cos(freqs).astype(config.param_dtype), jnp.sin(freqs).astype(config.param_dtype)\n",
    "\n",
    "def apply_rotary_emb(qkv: Array, cos: Array, sin: Array) -> Array:\n",
    "    _, seq_len, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "\n",
    "    q1, q2 = jnp.split(q_rot.astype(jnp.float32), 2, axis=-1)\n",
    "    k1, k2 = jnp.split(k_rot.astype(jnp.float32), 2, axis=-1)\n",
    "    c, s = cos[:seq_len][:, None, :].astype(jnp.float32), sin[:seq_len][:, None, :].astype(jnp.float32)\n",
    "\n",
    "    q_rot = jnp.concatenate([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1).astype(qkv.dtype)\n",
    "    k_rot = jnp.concatenate([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).astype(qkv.dtype)\n",
    "\n",
    "    return jnp.concatenate([\n",
    "        jnp.concatenate([q_rot, q_pass], axis=-1)[:, :, None, :, :],\n",
    "        jnp.concatenate([k_rot, k_pass], axis=-1)[:, :, None, :, :],\n",
    "        qkv[:, :, 2:3, :, :]\n",
    "    ], axis=2)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> Array:\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "\n",
    "        with ensure_compile_time_eval():\n",
    "            cos, sin = compute_cos_sin(self.config)\n",
    "\n",
    "        scale = (n_embed // self.config.n_head) ** -0.5\n",
    "        mask = jnp.triu(jnp.full((seq_len, seq_len), -10000.0, dtype=jnp.float16), 1)\n",
    "        qkv = nn.Dense(features=3 * self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        qkv = jnp.reshape(qkv, (batch_size, seq_len, 3, self.config.n_head, n_embed // self.config.n_head))\n",
    "        qkv = apply_rotary_emb(qkv, cos, sin)\n",
    "        qkv = jnp.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = jnp.split(qkv, 3, axis=0)\n",
    "        a = (q @ jnp.swapaxes(k, -2, -1)) * scale + mask\n",
    "        a = nn.softmax(a, axis=-1)\n",
    "        a = (a @ v).swapaxes(1, 2).reshape(batch_size, seq_len, n_embed)\n",
    "        return nn.Dense(features=n_embed, use_bias=True, param_dtype=self.config.param_dtype)(a)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> (Array, Array):\n",
    "        h = nn.Dense(features=self.config.n_embed * 4, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        h = nn.Dense(features=self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(nn.gelu(h))\n",
    "\n",
    "        l = nn.Dense(features=self.config.target_hidden_size, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        l = nn.Dense(features=self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(nn.gelu(l))\n",
    "\n",
    "        return (h, l2_loss(l, h))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> (Array, Array):\n",
    "        h = nn.LayerNorm(epsilon=self.config.ln_eps, param_dtype=self.config.param_dtype)(x)\n",
    "        a = SelfAttention(self.config)(h)\n",
    "        (h, loss) = MLP(self.config)(h)\n",
    "        return (a + h, loss)\n",
    "\n",
    "class Phi(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> list[Array]:\n",
    "        losses = []\n",
    "        h = nn.Embed(num_embeddings=self.config.vocab_size, features=self.config.n_embed, param_dtype=self.config.param_dtype)(x)\n",
    "        for _ in range(self.config.n_layer):\n",
    "            (h, loss) = Block(self.config)(h)\n",
    "            losses.append(loss)\n",
    "        # useless layers while training\n",
    "        # h = nn.LayerNorm(epsilon=self.config.ln_eps, param_dtype=self.config.param_dtype)(h)\n",
    "        # o = nn.Dense(self.config.vocab_size, use_bias=True, param_dtype=self.config.param_dtype)(h)\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_into_flax(model_path, variables) -> dict:\n",
    "    model = load(model_path)\n",
    "    j = 0\n",
    "    for param_name in model:\n",
    "        param_name = param_name.replace(\"layers.\", \"\")\n",
    "        i = int(param_name.split(\".\")[0])\n",
    "        jnp_array = jnp.array(model[param_name].numpy()).astype(jnp.float16)\n",
    "        if i == 0:\n",
    "            variables[\"params\"][f\"Embed_{i}\"][\"embedding\"] = jnp_array\n",
    "        elif not param_name.endswith(\"inv_freq\"):\n",
    "            match i:\n",
    "                case 0: variables[\"params\"][f\"Block_{i}\"][\"LayerNorm_0\"][\"scale\"] = jnp_array; j += 1\n",
    "                case 1: variables[\"params\"][f\"Block_{i}\"][\"LayerNorm_0\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 2: variables[\"params\"][f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array; j += 1\n",
    "                case 3: variables[\"params\"][f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 4: variables[\"params\"][f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array; j += 1\n",
    "                case 5: variables[\"params\"][f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 6: variables[\"params\"][f\"Block_{i}\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array; j += 1\n",
    "                case 7: variables[\"params\"][f\"Block_{i}\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 8: variables[\"params\"][f\"Block_{i}\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array; j += 1\n",
    "                case 9: variables[\"params\"][f\"Block_{i}\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 10: j = 0\n",
    "    return variables\n",
    "\n",
    "def init_train_state(batch_size, model_path) -> TrainState:\n",
    "    config = PhiConfig()\n",
    "    phi = Phi(config)\n",
    "    variables = phi.init(random.PRNGKey(0), jnp.ones((batch_size, config.n_positions), dtype=jnp.int32))\n",
    "    # variables = load_model_into_flax(model_path, variables)\n",
    "\n",
    "    partition_optimizers = {\"trainable\": adamw(), \"frozen\": set_to_zero()}\n",
    "    param_partitions = traverse_util.path_aware_map(\n",
    "        lambda path, _: \"trainable\" if (\"Dense_2\" in path or  \"Dense_3\" in path) else \"frozen\", variables[\"params\"])\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=phi.apply,\n",
    "        tx=multi_transform(partition_optimizers, param_partitions),\n",
    "        params=variables[\"params\"]\n",
    "    )\n",
    "    state = replicate(state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pmap\n",
    "def train_step(state: TrainState, batch: Array):\n",
    "    def loss_fn(params):\n",
    "        return state.apply_fn({\"params\": params}, batch)\n",
    "    \n",
    "    grad_fn = value_and_grad(loss_fn)\n",
    "    _, grads = grad_fn(state.params)\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    n = 10\n",
    "    batch_size = 16\n",
    "    train_iter = data_iter(n, 2)\n",
    "    state = init_train_state(batch_size, \"\")\n",
    "\n",
    "    for batch in tqdm(train_iter):\n",
    "        state = train_step(state, batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from tqdm import tqdm\n",
    "from torch import load\n",
    "from requests import get\n",
    "from jax.lax import pmean\n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "from flax import traverse_util\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from flax.jax_utils import replicate\n",
    "from transformers import AutoTokenizer\n",
    "from jax import Array, pmap, jit, random\n",
    "from jax.nn.initializers import lecun_normal\n",
    "from flax.training.train_state import TrainState\n",
    "from optax import adamw, set_to_zero, multi_transform\n",
    "from jax import numpy as jnp, ensure_compile_time_eval\n",
    "from flax.training.orbax_utils import save_args_from_target\n",
    "from orbax.checkpoint import Checkpointer, PyTreeCheckpointHandler\n",
    "from jax import numpy as jnp, value_and_grad, ensure_compile_time_eval\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PhiConfig:\n",
    "    n_head: int = 32\n",
    "    n_layer: int = 24\n",
    "    n_embed: int = 2048\n",
    "    rotary_dim: int = 32\n",
    "    ln_eps: float = 1e-5\n",
    "    n_positions: int = 2048\n",
    "    vocab_size: int = 51200\n",
    "    target_hidden_size: int = 2048\n",
    "    param_dtype: jnp.dtype = jnp.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cos_sin(config: PhiConfig) -> (Array, Array):\n",
    "    t = jnp.arange(config.n_positions, dtype=jnp.float32)\n",
    "    inv_freq = 1 / (10000 ** (jnp.arange(0, config.rotary_dim, 2, dtype=jnp.float32) / config.rotary_dim))\n",
    "    freqs = jnp.outer(t, inv_freq)\n",
    "    return jnp.cos(freqs).astype(config.param_dtype), jnp.sin(freqs).astype(config.param_dtype)\n",
    "\n",
    "def apply_rotary_emb(qkv: Array, cos: Array, sin: Array) -> Array:\n",
    "    _, seq_len, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "\n",
    "    q1, q2 = jnp.split(q_rot.astype(jnp.float32), 2, axis=-1)\n",
    "    k1, k2 = jnp.split(k_rot.astype(jnp.float32), 2, axis=-1)\n",
    "    c, s = cos[:seq_len][:, None, :].astype(jnp.float32), sin[:seq_len][:, None, :].astype(jnp.float32)\n",
    "\n",
    "    q_rot = jnp.concatenate([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1).astype(qkv.dtype)\n",
    "    k_rot = jnp.concatenate([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).astype(qkv.dtype)\n",
    "\n",
    "    return jnp.concatenate([\n",
    "        jnp.concatenate([q_rot, q_pass], axis=-1)[:, :, None, :, :],\n",
    "        jnp.concatenate([k_rot, k_pass], axis=-1)[:, :, None, :, :],\n",
    "        qkv[:, :, 2:3, :, :]\n",
    "    ], axis=2)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> Array:\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "\n",
    "        with ensure_compile_time_eval():\n",
    "            cos, sin = compute_cos_sin(self.config)\n",
    "\n",
    "        scale = (n_embed // self.config.n_head) ** -0.5\n",
    "        mask = jnp.triu(jnp.full((seq_len, seq_len), -10000.0, dtype=jnp.float16), 1)\n",
    "        qkv = nn.Dense(features=3 * self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        qkv = jnp.reshape(qkv, (batch_size, seq_len, 3, self.config.n_head, n_embed // self.config.n_head))\n",
    "        qkv = apply_rotary_emb(qkv, cos, sin)\n",
    "        qkv = jnp.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = jnp.split(qkv, 3, axis=0)\n",
    "        a = (q @ jnp.swapaxes(k, -2, -1)) * scale + mask\n",
    "        a = nn.softmax(a, axis=-1)\n",
    "        a = (a @ v).swapaxes(1, 2).reshape(batch_size, seq_len, n_embed)\n",
    "        return nn.Dense(features=n_embed, use_bias=True, param_dtype=self.config.param_dtype)(a)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> (Array, Array):\n",
    "        h = nn.Dense(features=self.config.n_embed * 4, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        h = nn.Dense(features=self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(nn.gelu(h))\n",
    "\n",
    "        l = nn.Dense(features=self.config.target_hidden_size, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        l = nn.Dense(features=self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(nn.gelu(l))\n",
    "        return (h, jnp.mean((l - h)**2))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> (Array, Array):\n",
    "        h = nn.LayerNorm(epsilon=self.config.ln_eps, param_dtype=self.config.param_dtype)(x)\n",
    "        a = SelfAttention(self.config)(h)\n",
    "        (h, loss) = MLP(self.config)(h)\n",
    "        return (a + h, loss)\n",
    "\n",
    "class Phi(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> (Array, Array, Array):\n",
    "        losses = 0\n",
    "        h = nn.Embed(num_embeddings=self.config.vocab_size, features=self.config.n_embed, param_dtype=self.config.param_dtype)(x)\n",
    "        for _ in range(self.config.n_layer):\n",
    "            (h, loss) = Block(self.config)(h)\n",
    "            losses += loss\n",
    "        # useless layers while training\n",
    "        h = nn.LayerNorm(epsilon=self.config.ln_eps, param_dtype=self.config.param_dtype)(h)\n",
    "        h = nn.Dense(self.config.vocab_size, use_bias=True, param_dtype=self.config.param_dtype)(h)\n",
    "        return (h, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_into_flax(config, model_path) -> dict:\n",
    "    print(\"loading pytorch model\")\n",
    "    with open(\"/kaggle/working/pytorch_model.bin\", \"wb\") as f:\n",
    "        f.write(get(model_path, allow_redirects=True).content)\n",
    "    model = load(\"/kaggle/working/pytorch_model.bin\", map_location=\"cpu\")\n",
    "    print(\"pytorch model loaded\")\n",
    "    initializer = lecun_normal()\n",
    "    print(\"loading model into flax\")\n",
    "    print(\"init trainable params\")\n",
    "    params = {}\n",
    "    params[\"Embed_0\"] = {}\n",
    "    params[\"LayerNorm_0\"] = {}\n",
    "    params[\"Dense_0\"] = {}\n",
    "    for i in range(config.n_layer):\n",
    "        params[f\"Block_{i}\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_2\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_3\"] = {}\n",
    "        params[f\"Block_{i}\"][\"LayerNorm_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"SelfAttention_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_1\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_1\"] = {}\n",
    "\n",
    "    for i in range(config.n_layer):\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_2\"][\"kernel\"] = initializer(random.PRNGKey(0), (config.n_embed, config.target_hidden_size), dtype=config.param_dtype)\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_2\"][\"bias\"] = jnp.zeros((config.target_hidden_size,), dtype=config.param_dtype)\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_3\"][\"kernel\"] = initializer(random.PRNGKey(0), (config.target_hidden_size, config.n_embed), dtype=config.param_dtype)\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_3\"][\"bias\"] = jnp.zeros((config.n_embed,), dtype=config.param_dtype)\n",
    "    print(\"init frozen params\")\n",
    "    for param_name in model:\n",
    "        jnp_array = jnp.array(model[param_name].numpy()).astype(jnp.float16)\n",
    "        if \"transformer\" in param_name:\n",
    "            if \"embd\" in param_name:\n",
    "                params[\"Embed_0\"][\"embedding\"] = jnp_array\n",
    "            else:\n",
    "                param_name = param_name.replace(\"transformer.h\", \"\")\n",
    "                layer = int(param_name.split(\".\")[1])\n",
    "                param_name = param_name.replace(f\".{layer}.\", \"\")\n",
    "                match param_name:\n",
    "                    case \"ln.weight\": params[f\"Block_{layer}\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "                    case \"ln.bias\": params[f\"Block_{layer}\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "                    case \"mixer.Wqkv.weight\": params[f\"Block_{layer}\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array.T\n",
    "                    case \"mixer.Wqkv.bias\": params[f\"Block_{layer}\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "                    case \"mixer.out_proj.weight\": params[f\"Block_{layer}\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "                    case \"mixer.out_proj.bias\": params[f\"Block_{layer}\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "                    case \"mlp.fc1.weight\": params[f\"Block_{layer}\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array.T\n",
    "                    case \"mlp.fc1.bias\": params[f\"Block_{layer}\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "                    case \"mlp.fc2.weight\": params[f\"Block_{layer}\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array.T\n",
    "                    case \"mlp.fc2.bias\": params[f\"Block_{layer}\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "        else:\n",
    "            param_name = param_name.replace(\"lm_head.\", \"\")\n",
    "            match param_name:\n",
    "                case \"ln.weight\": params[\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "                case \"ln.bias\": params[\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "                case \"linear.weight\": params[\"Dense_0\"][\"kernel\"] = jnp_array.T\n",
    "                case \"linear.bias\": params[\"Dense_0\"][\"bias\"] = jnp_array\n",
    "    print(\"model loaded into flax\")\n",
    "    return params\n",
    "\n",
    "def init_train_state(config, model_path) -> TrainState:\n",
    "    phi = Phi(config)\n",
    "    params = load_model_into_flax(config, model_path)\n",
    "\n",
    "    partition_optimizers = {\"trainable\": adamw(3e-4), \"frozen\": set_to_zero()}\n",
    "    param_partitions = traverse_util.path_aware_map(lambda path, _: \"trainable\" if \"Dense_2\" in path else \"frozen\", params)\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=phi.apply,\n",
    "        tx=multi_transform(partition_optimizers, param_partitions),\n",
    "        params=params\n",
    "    )\n",
    "    return replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(pmap, axis_name=\"device\")\n",
    "def train_step(state: TrainState, batch: Array):\n",
    "    def loss_fn(params):\n",
    "        _, loss = state.apply_fn({\"params\": params}, batch)\n",
    "        return loss\n",
    "    \n",
    "    grad_fn = value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    grads = pmean(grads, axis_name=\"device\")\n",
    "    loss = pmean(loss, axis_name=\"device\")\n",
    "    return (state.apply_gradients(grads=grads), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state: TrainState, n_iter: int, batch_size: int, config: PhiConfig) -> TrainState:\n",
    "    dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_gen\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    n_devices = jax.local_device_count()\n",
    "\n",
    "    loss = 0\n",
    "    tqdm_range = tqdm(range(n_iter))\n",
    "    for i in tqdm_range:\n",
    "        batch = jnp.array(tokenizer([dataset[i * batch_size + j][\"prompt\"] for j in range(batch_size * n_devices)], padding=\"max_length\", max_length=config.n_positions // 2, truncation=True)[\"input_ids\"], dtype=jnp.int32)\n",
    "        batch = jax.tree_map(lambda x: x.reshape((n_devices, -1, *x.shape[1:])), batch)\n",
    "\n",
    "        state, loss = train_step(state, batch)\n",
    "        tqdm_range.set_description(f\"Loss: {loss.mean():.4f}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "config = PhiConfig()\n",
    "state = init_train_state(config, \"https://huggingface.co/microsoft/phi-1_5/resolve/main/pytorch_model.bin\")\n",
    "state = train_epoch(state, 500, batch_size, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

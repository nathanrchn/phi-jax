{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from torch import load\n",
    "from requests import get\n",
    "from jax.lax import pmean\n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "from flax import traverse_util\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from flax.jax_utils import replicate\n",
    "from transformers import AutoTokenizer\n",
    "from jax import Array, pmap, jit, random\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.training.checkpoints import save_checkpoint\n",
    "from optax import adamw, set_to_zero, multi_transform\n",
    "from jax import numpy as jnp, ensure_compile_time_eval\n",
    "from jax import numpy as jnp, value_and_grad, ensure_compile_time_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PhiConfig:\n",
    "    n_head: int = 32\n",
    "    n_layer: int = 24\n",
    "    n_embed: int = 2048\n",
    "    rotary_dim: int = 32\n",
    "    ln_eps: float = 1e-5\n",
    "    n_positions: int = 2048\n",
    "    vocab_size: int = 51200\n",
    "    target_hidden_size: int = 2048\n",
    "    param_dtype: jnp.dtype = jnp.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cos_sin(config: PhiConfig) -> (Array, Array):\n",
    "    t = jnp.arange(config.n_positions, dtype=jnp.float32)\n",
    "    inv_freq = 1 / (10000 ** (jnp.arange(0, config.rotary_dim, 2, dtype=jnp.float32) / config.rotary_dim))\n",
    "    freqs = jnp.outer(t, inv_freq)\n",
    "    return jnp.cos(freqs).astype(config.param_dtype), jnp.sin(freqs).astype(config.param_dtype)\n",
    "\n",
    "def apply_rotary_emb(qkv: Array, cos: Array, sin: Array) -> Array:\n",
    "    _, seq_len, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "\n",
    "    q1, q2 = jnp.split(q_rot.astype(jnp.float32), 2, axis=-1)\n",
    "    k1, k2 = jnp.split(k_rot.astype(jnp.float32), 2, axis=-1)\n",
    "    c, s = cos[:seq_len][:, None, :].astype(jnp.float32), sin[:seq_len][:, None, :].astype(jnp.float32)\n",
    "\n",
    "    q_rot = jnp.concatenate([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1).astype(qkv.dtype)\n",
    "    k_rot = jnp.concatenate([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).astype(qkv.dtype)\n",
    "\n",
    "    return jnp.concatenate([\n",
    "        jnp.concatenate([q_rot, q_pass], axis=-1)[:, :, None, :, :],\n",
    "        jnp.concatenate([k_rot, k_pass], axis=-1)[:, :, None, :, :],\n",
    "        qkv[:, :, 2:3, :, :]\n",
    "    ], axis=2)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> Array:\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "\n",
    "        with ensure_compile_time_eval():\n",
    "            cos, sin = compute_cos_sin(self.config)\n",
    "\n",
    "        scale = (n_embed // self.config.n_head) ** -0.5\n",
    "        mask = jnp.triu(jnp.full((seq_len, seq_len), -10000.0, dtype=jnp.float16), 1)\n",
    "        qkv = nn.Dense(features=3 * self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        qkv = jnp.reshape(qkv, (batch_size, seq_len, 3, self.config.n_head, n_embed // self.config.n_head))\n",
    "        qkv = apply_rotary_emb(qkv, cos, sin)\n",
    "        qkv = jnp.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = jnp.split(qkv, 3, axis=0)\n",
    "        a = (q @ jnp.swapaxes(k, -2, -1)) * scale + mask\n",
    "        a = nn.softmax(a, axis=-1)\n",
    "        a = (a @ v).swapaxes(1, 2).reshape(batch_size, seq_len, n_embed)\n",
    "        return nn.Dense(features=n_embed, use_bias=True, param_dtype=self.config.param_dtype)(a)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> (Array, Array):\n",
    "        h = nn.Dense(features=self.config.n_embed * 4, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        h = nn.Dense(features=self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(nn.gelu(h))\n",
    "\n",
    "        l = nn.Dense(features=self.config.target_hidden_size, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        l = nn.Dense(features=self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(nn.gelu(l))\n",
    "\n",
    "        return (h, jnp.mean((l - h)**2))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> (Array, Array):\n",
    "        h = nn.LayerNorm(epsilon=self.config.ln_eps, param_dtype=self.config.param_dtype)(x)\n",
    "        a = SelfAttention(self.config)(h)\n",
    "        (h, loss) = MLP(self.config)(h)\n",
    "        return (a + h, loss)\n",
    "\n",
    "class Phi(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> list[Array]:\n",
    "        losses = 0\n",
    "        h = nn.Embed(num_embeddings=self.config.vocab_size, features=self.config.n_embed, param_dtype=self.config.param_dtype)(x)\n",
    "        for _ in range(self.config.n_layer):\n",
    "            (h, loss) = Block(self.config)(h)\n",
    "            losses += loss\n",
    "        # useless layers while training\n",
    "        # h = nn.LayerNorm(epsilon=self.config.ln_eps, param_dtype=self.config.param_dtype)(h)\n",
    "        # o = nn.Dense(self.config.vocab_size, use_bias=True, param_dtype=self.config.param_dtype)(h)\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_into_flax(model_path, variables) -> dict:\n",
    "    model = load(get(model_path, allow_redirects=True).content)\n",
    "    j = 0\n",
    "    for param_name in model:\n",
    "        param_name = param_name.replace(\"layers.\", \"\")\n",
    "        i = int(param_name.split(\".\")[0])\n",
    "        jnp_array = jnp.array(model[param_name].numpy()).astype(jnp.float16)\n",
    "        if i == 0:\n",
    "            variables[\"params\"][f\"Embed_{i}\"][\"embedding\"] = jnp_array\n",
    "        elif not param_name.endswith(\"inv_freq\"):\n",
    "            match i:\n",
    "                case 0: variables[\"params\"][f\"Block_{i}\"][\"LayerNorm_0\"][\"scale\"] = jnp_array; j += 1\n",
    "                case 1: variables[\"params\"][f\"Block_{i}\"][\"LayerNorm_0\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 2: variables[\"params\"][f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array; j += 1\n",
    "                case 3: variables[\"params\"][f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 4: variables[\"params\"][f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array; j += 1\n",
    "                case 5: variables[\"params\"][f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 6: variables[\"params\"][f\"Block_{i}\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array; j += 1\n",
    "                case 7: variables[\"params\"][f\"Block_{i}\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 8: variables[\"params\"][f\"Block_{i}\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array; j += 1\n",
    "                case 9: variables[\"params\"][f\"Block_{i}\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array; j += 1\n",
    "                case 10: j = 0\n",
    "    return variables\n",
    "\n",
    "def init_train_state(config, batch_size, model_path) -> TrainState:\n",
    "    phi = Phi(config)\n",
    "    variables = phi.init(random.PRNGKey(0), jnp.ones((batch_size // jax.local_device_count(), config.n_positions), dtype=jnp.int32))\n",
    "    variables = load_model_into_flax(model_path, variables)\n",
    "\n",
    "    partition_optimizers = {\"trainable\": adamw(3e-4), \"frozen\": set_to_zero()}\n",
    "    param_partitions = traverse_util.path_aware_map(\n",
    "        lambda path, _: \"trainable\" if (\"Dense_2\" in path or  \"Dense_3\" in path) else \"frozen\", variables[\"params\"])\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=phi.apply,\n",
    "        tx=multi_transform(partition_optimizers, param_partitions),\n",
    "        params=variables[\"params\"]\n",
    "    )\n",
    "    return replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, axis_name=\"device\")\n",
    "def train_step(state: TrainState, batch: Array):\n",
    "    def loss_fn(params):\n",
    "        losses = state.apply_fn({\"params\": params}, batch)\n",
    "        return losses\n",
    "    \n",
    "    grad_fn = value_and_grad(loss_fn)\n",
    "    _, grads = grad_fn(state.params)\n",
    "    grads = pmean(grads, axis_name=\"device\")\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state: TrainState, n_iter: int, batch_size: int, config: PhiConfig) -> TrainState:\n",
    "    dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")[\"train_sft\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        batch = jnp.array(tokenizer([dataset[i * batch_size + j][\"prompt\"] for j in range(batch_size)], padding=\"max_length\", max_length=config.n_positions // 2, truncation=True)[\"input_ids\"], dtype=jnp.int32)\n",
    "        batch = jax.tree_map(lambda x: x.reshape((jax.local_device_count(), -1, *x.shape[1:])), batch)\n",
    "\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    return state\n",
    "        \n",
    "def train(batch_size):\n",
    "    config = PhiConfig()\n",
    "    state = init_train_state(config, batch_size, \"https://huggingface.co/microsoft/phi-1_5/resolve/main/pytorch_model.bin\")\n",
    "    state = train_epoch(state, 10, batch_size, config)\n",
    "    save_checkpoint(\"phi_1_5\", state, 0)\n",
    "    \n",
    "train(16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from torch import load\n",
    "from requests import get\n",
    "from jax.lax import pmean\n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "from flax import traverse_util\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from flax.jax_utils import replicate\n",
    "from transformers import AutoTokenizer\n",
    "from jax import Array, pmap, jit, random\n",
    "from  jax.nn.initializers import lecun_normal\n",
    "from flax.training.train_state import TrainState\n",
    "from optax import adamw, set_to_zero, multi_transform\n",
    "from jax import numpy as jnp, ensure_compile_time_eval\n",
    "from flax.training.orbax_utils import save_args_from_target\n",
    "from orbax.checkpoint import Checkpointer, PyTreeCheckpointHandler\n",
    "from jax import numpy as jnp, value_and_grad, ensure_compile_time_eval\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PhiConfig:\n",
    "    n_head: int = 32\n",
    "    n_layer: int = 24\n",
    "    n_embed: int = 2048\n",
    "    rotary_dim: int = 32\n",
    "    ln_eps: float = 1e-5\n",
    "    n_positions: int = 2048\n",
    "    vocab_size: int = 51200\n",
    "    target_hidden_size: int = 2048\n",
    "    param_dtype: jnp.dtype = jnp.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cos_sin(config: PhiConfig) -> (Array, Array):\n",
    "    t = jnp.arange(config.n_positions, dtype=jnp.float32)\n",
    "    inv_freq = 1 / (10000 ** (jnp.arange(0, config.rotary_dim, 2, dtype=jnp.float32) / config.rotary_dim))\n",
    "    freqs = jnp.outer(t, inv_freq)\n",
    "    return jnp.cos(freqs).astype(config.param_dtype), jnp.sin(freqs).astype(config.param_dtype)\n",
    "\n",
    "def apply_rotary_emb(qkv: Array, cos: Array, sin: Array) -> Array:\n",
    "    _, seq_len, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "\n",
    "    q1, q2 = jnp.split(q_rot.astype(jnp.float32), 2, axis=-1)\n",
    "    k1, k2 = jnp.split(k_rot.astype(jnp.float32), 2, axis=-1)\n",
    "    c, s = cos[:seq_len][:, None, :].astype(jnp.float32), sin[:seq_len][:, None, :].astype(jnp.float32)\n",
    "\n",
    "    q_rot = jnp.concatenate([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1).astype(qkv.dtype)\n",
    "    k_rot = jnp.concatenate([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).astype(qkv.dtype)\n",
    "\n",
    "    return jnp.concatenate([\n",
    "        jnp.concatenate([q_rot, q_pass], axis=-1)[:, :, None, :, :],\n",
    "        jnp.concatenate([k_rot, k_pass], axis=-1)[:, :, None, :, :],\n",
    "        qkv[:, :, 2:3, :, :]\n",
    "    ], axis=2)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> Array:\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "\n",
    "        with ensure_compile_time_eval():\n",
    "            cos, sin = compute_cos_sin(self.config)\n",
    "\n",
    "        scale = (n_embed // self.config.n_head) ** -0.5\n",
    "        mask = jnp.triu(jnp.full((seq_len, seq_len), -10000.0, dtype=jnp.float16), 1)\n",
    "        qkv = nn.Dense(features=3 * self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        qkv = jnp.reshape(qkv, (batch_size, seq_len, 3, self.config.n_head, n_embed // self.config.n_head))\n",
    "        qkv = apply_rotary_emb(qkv, cos, sin)\n",
    "        qkv = jnp.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = jnp.split(qkv, 3, axis=0)\n",
    "        a = (q @ jnp.swapaxes(k, -2, -1)) * scale + mask\n",
    "        a = nn.softmax(a, axis=-1)\n",
    "        a = (a @ v).swapaxes(1, 2).reshape(batch_size, seq_len, n_embed)\n",
    "        return nn.Dense(features=n_embed, use_bias=True, param_dtype=self.config.param_dtype)(a)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> (Array, Array):\n",
    "        h = nn.Dense(features=self.config.n_embed * 4, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        h = nn.Dense(features=self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(nn.gelu(h))\n",
    "\n",
    "        l = nn.Dense(features=self.config.target_hidden_size, use_bias=True, param_dtype=self.config.param_dtype)(x)\n",
    "        l = nn.Dense(features=self.config.n_embed, use_bias=True, param_dtype=self.config.param_dtype)(nn.gelu(l))\n",
    "\n",
    "        return (h, jnp.mean((l - h)**2))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> (Array, Array):\n",
    "        h = nn.LayerNorm(epsilon=self.config.ln_eps, param_dtype=self.config.param_dtype)(x)\n",
    "        a = SelfAttention(self.config)(h)\n",
    "        (h, loss) = MLP(self.config)(h)\n",
    "        return (a + h, loss)\n",
    "\n",
    "class Phi(nn.Module):\n",
    "    config: PhiConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array) -> list[Array]:\n",
    "        total_loss = 0\n",
    "        h = nn.Embed(num_embeddings=self.config.vocab_size, features=self.config.n_embed, param_dtype=self.config.param_dtype)(x)\n",
    "        for _ in range(self.config.n_layer):\n",
    "            (h, loss) = Block(self.config)(h)\n",
    "            total_loss += loss\n",
    "        # useless layers while training\n",
    "        h = nn.LayerNorm(epsilon=self.config.ln_eps, param_dtype=self.config.param_dtype)(h)\n",
    "        o = nn.Dense(self.config.vocab_size, use_bias=True, param_dtype=self.config.param_dtype)(h)\n",
    "        return o, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_into_flax(config, model_path) -> dict:\n",
    "    print(\"loading pytorch model\")\n",
    "    with open(\"/kaggle/working/pytorch_model.bin\", \"wb\") as f:\n",
    "        f.write(get(model_path, allow_redirects=True).content)\n",
    "    model = load(\"/kaggle/working/pytorch_model.bin\", map_location=\"cpu\")\n",
    "    print(\"pytorch model loaded\")\n",
    "    initializer = lecun_normal()\n",
    "    print(\"loading model into flax\")\n",
    "    print(\"init trainable params\")\n",
    "    params = {}\n",
    "    for i in range(config.n_layer):\n",
    "        params[f\"Embed_{i}\"] = {}\n",
    "        params[\"LayerNorm_0\"] = {}\n",
    "        params[\"Dense_0\"] = {}\n",
    "        params[f\"Block_{i}\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_2\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_3\"] = {}\n",
    "        params[f\"Block_{i}\"][\"LayerNorm_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"SelfAttention_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"SelfAttention_0\"][\"Dense_1\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_0\"] = {}\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_1\"] = {}\n",
    "\n",
    "    for i in range(config.n_layer):\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_2\"][\"kernel\"] = initializer(random.PRNGKey(0), (config.n_embed, config.target_hidden_size), dtype=config.param_dtype)\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_2\"][\"bias\"] = jnp.zeros((config.target_hidden_size,), dtype=config.param_dtype)\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_3\"][\"kernel\"] = initializer(random.PRNGKey(0), (config.target_hidden_size, config.n_embed), dtype=config.param_dtype)\n",
    "        params[f\"Block_{i}\"][\"MLP_0\"][\"Dense_3\"][\"bias\"] = jnp.zeros((config.n_embed,), dtype=config.param_dtype)\n",
    "    print(\"init frozen params\")\n",
    "    for param_name in model:\n",
    "        if \"layers\" not in param_name:\n",
    "            continue\n",
    "        param_name = param_name.replace(\"transformer.\", \"\")\n",
    "        jnp_array = jnp.array(model[param_name].numpy()).astype(jnp.float16)\n",
    "        match param_name:\n",
    "            case \"embd.wte.weight\": params[\"Embed_0\"][\"embedding\"] = jnp_array\n",
    "            case \"1.ln.weight\": params[\"Block_0\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"1.ln.bias\": params[\"Block_0\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"1.mixer.Wqkv.weight\": params[\"Block_0\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"1.mixer.Wqkv.bias\": params[\"Block_0\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"1.mixer.out_proj.weight\": params[\"Block_0\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"1.mixer.out_proj.bias\": params[\"Block_0\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"1.mlp.fc1.weight\": params[\"Block_0\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"1.mlp.fc1.bias\": params[\"Block_0\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"1.mlp.fc2.weight\": params[\"Block_0\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"1.mlp.fc2.bias\": params[\"Block_0\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"2.ln.weight\": params[\"Block_1\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"2.ln.bias\": params[\"Block_1\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"2.mixer.Wqkv.weight\": params[\"Block_1\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"2.mixer.Wqkv.bias\": params[\"Block_1\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"2.mixer.out_proj.weight\": params[\"Block_1\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"2.mixer.out_proj.bias\": params[\"Block_1\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"2.mlp.fc1.weight\": params[\"Block_1\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"2.mlp.fc1.bias\": params[\"Block_1\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"2.mlp.fc2.weight\": params[\"Block_1\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"2.mlp.fc2.bias\": params[\"Block_1\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"3.ln.weight\": params[\"Block_2\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"3.ln.bias\": params[\"Block_2\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"3.mixer.Wqkv.weight\": params[\"Block_2\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"3.mixer.Wqkv.bias\": params[\"Block_2\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"3.mixer.out_proj.weight\": params[\"Block_2\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"3.mixer.out_proj.bias\": params[\"Block_2\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"3.mlp.fc1.weight\": params[\"Block_2\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"3.mlp.fc1.bias\": params[\"Block_2\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"3.mlp.fc2.weight\": params[\"Block_2\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"3.mlp.fc2.bias\": params[\"Block_2\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"4.ln.weight\": params[\"Block_3\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"4.ln.bias\": params[\"Block_3\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"4.mixer.Wqkv.weight\": params[\"Block_3\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"4.mixer.Wqkv.bias\": params[\"Block_3\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"4.mixer.out_proj.weight\": params[\"Block_3\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"4.mixer.out_proj.bias\": params[\"Block_3\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"4.mlp.fc1.weight\": params[\"Block_3\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"4.mlp.fc1.bias\": params[\"Block_3\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"4.mlp.fc2.weight\": params[\"Block_3\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"4.mlp.fc2.bias\": params[\"Block_3\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"5.ln.weight\": params[\"Block_4\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"5.ln.bias\": params[\"Block_4\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"5.mixer.Wqkv.weight\": params[\"Block_4\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"5.mixer.Wqkv.bias\": params[\"Block_4\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"5.mixer.out_proj.weight\": params[\"Block_4\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"5.mixer.out_proj.bias\": params[\"Block_4\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"5.mlp.fc1.weight\": params[\"Block_4\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"5.mlp.fc1.bias\": params[\"Block_4\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"5.mlp.fc2.weight\": params[\"Block_4\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"5.mlp.fc2.bias\": params[\"Block_4\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"6.ln.weight\": params[\"Block_5\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"6.ln.bias\": params[\"Block_5\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"6.mixer.Wqkv.weight\": params[\"Block_5\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"6.mixer.Wqkv.bias\": params[\"Block_5\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"6.mixer.out_proj.weight\": params[\"Block_5\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"6.mixer.out_proj.bias\": params[\"Block_5\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"6.mlp.fc1.weight\": params[\"Block_5\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"6.mlp.fc1.bias\": params[\"Block_5\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"6.mlp.fc2.weight\": params[\"Block_5\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"6.mlp.fc2.bias\": params[\"Block_5\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"8.ln.weight\": params[\"Block_7\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"8.ln.bias\": params[\"Block_7\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"8.mixer.Wqkv.weight\": params[\"Block_7\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"8.mixer.Wqkv.bias\": params[\"Block_7\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"8.mixer.out_proj.weight\": params[\"Block_7\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"8.mixer.out_proj.bias\": params[\"Block_7\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"8.mlp.fc1.weight\": params[\"Block_7\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"8.mlp.fc1.bias\": params[\"Block_7\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"8.mlp.fc2.weight\": params[\"Block_7\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"8.mlp.fc2.bias\": params[\"Block_7\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"9.ln.weight\": params[\"Block_8\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"9.ln.bias\": params[\"Block_8\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"9.mixer.Wqkv.weight\": params[\"Block_8\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"9.mixer.Wqkv.bias\": params[\"Block_8\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"9.mixer.out_proj.weight\": params[\"Block_8\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"9.mixer.out_proj.bias\": params[\"Block_8\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"9.mlp.fc1.weight\": params[\"Block_8\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"9.mlp.fc1.bias\": params[\"Block_8\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"9.mlp.fc2.weight\": params[\"Block_8\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"9.mlp.fc2.bias\": params[\"Block_8\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"10.ln.weight\": params[\"Block_9\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"10.ln.bias\": params[\"Block_9\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"10.mixer.Wqkv.weight\": params[\"Block_9\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"10.mixer.Wqkv.bias\": params[\"Block_9\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"10.mixer.out_proj.weight\": params[\"Block_9\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"10.mixer.out_proj.bias\": params[\"Block_9\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"10.mlp.fc1.weight\": params[\"Block_9\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"10.mlp.fc1.bias\": params[\"Block_9\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"10.mlp.fc2.weight\": params[\"Block_9\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"10.mlp.fc2.bias\": params[\"Block_9\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"11.ln.weight\": params[\"Block_10\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"11.ln.bias\": params[\"Block_10\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"11.mixer.Wqkv.weight\": params[\"Block_10\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"11.mixer.Wqkv.bias\": params[\"Block_10\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"11.mixer.out_proj.weight\": params[\"Block_10\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"11.mixer.out_proj.bias\": params[\"Block_10\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"11.mlp.fc1.weight\": params[\"Block_10\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"11.mlp.fc1.bias\": params[\"Block_10\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"11.mlp.fc2.weight\": params[\"Block_10\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"11.mlp.fc2.bias\": params[\"Block_10\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"12.ln.weight\": params[\"Block_11\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"12.ln.bias\": params[\"Block_11\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"12.mixer.Wqkv.weight\": params[\"Block_11\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"12.mixer.Wqkv.bias\": params[\"Block_11\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"12.mixer.out_proj.weight\": params[\"Block_11\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"12.mixer.out_proj.bias\": params[\"Block_11\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"12.mlp.fc1.weight\": params[\"Block_11\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"12.mlp.fc1.bias\": params[\"Block_11\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"12.mlp.fc2.weight\": params[\"Block_11\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"12.mlp.fc2.bias\": params[\"Block_11\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"13.ln.weight\": params[\"Block_12\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"13.ln.bias\": params[\"Block_12\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"13.mixer.Wqkv.weight\": params[\"Block_12\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"13.mixer.Wqkv.bias\": params[\"Block_12\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"13.mixer.out_proj.weight\": params[\"Block_12\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"13.mixer.out_proj.bias\": params[\"Block_12\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"13.mlp.fc1.weight\": params[\"Block_12\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"13.mlp.fc1.bias\": params[\"Block_12\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"13.mlp.fc2.weight\": params[\"Block_12\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"13.mlp.fc2.bias\": params[\"Block_12\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"14.ln.weight\": params[\"Block_13\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"14.ln.bias\": params[\"Block_13\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"14.mixer.Wqkv.weight\": params[\"Block_13\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"14.mixer.Wqkv.bias\": params[\"Block_13\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"14.mixer.out_proj.weight\": params[\"Block_13\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"14.mixer.out_proj.bias\": params[\"Block_13\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"14.mlp.fc1.weight\": params[\"Block_13\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"14.mlp.fc1.bias\": params[\"Block_13\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"14.mlp.fc2.weight\": params[\"Block_13\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"14.mlp.fc2.bias\": params[\"Block_13\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"15.ln.weight\": params[\"Block_14\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"15.ln.bias\": params[\"Block_14\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"15.mixer.Wqkv.weight\": params[\"Block_14\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"15.mixer.Wqkv.bias\": params[\"Block_14\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"15.mixer.out_proj.weight\": params[\"Block_14\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"15.mixer.out_proj.bias\": params[\"Block_14\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"15.mlp.fc1.weight\": params[\"Block_14\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"15.mlp.fc1.bias\": params[\"Block_14\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"15.mlp.fc2.weight\": params[\"Block_14\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"15.mlp.fc2.bias\": params[\"Block_14\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"16.ln.weight\": params[\"Block_15\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"16.ln.bias\": params[\"Block_15\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"16.mixer.Wqkv.weight\": params[\"Block_15\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"16.mixer.Wqkv.bias\": params[\"Block_15\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"16.mixer.out_proj.weight\": params[\"Block_15\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"16.mixer.out_proj.bias\": params[\"Block_15\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"16.mlp.fc1.weight\": params[\"Block_15\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"16.mlp.fc1.bias\": params[\"Block_15\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"16.mlp.fc2.weight\": params[\"Block_15\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"16.mlp.fc2.bias\": params[\"Block_15\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"17.ln.weight\": params[\"Block_16\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"17.ln.bias\": params[\"Block_16\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"17.mixer.Wqkv.weight\": params[\"Block_16\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"17.mixer.Wqkv.bias\": params[\"Block_16\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"17.mixer.out_proj.weight\": params[\"Block_16\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"17.mixer.out_proj.bias\": params[\"Block_16\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"17.mlp.fc1.weight\": params[\"Block_16\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"17.mlp.fc1.bias\": params[\"Block_16\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"17.mlp.fc2.weight\": params[\"Block_16\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"17.mlp.fc2.bias\": params[\"Block_16\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"18.ln.weight\": params[\"Block_17\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"18.ln.bias\": params[\"Block_17\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"18.mixer.Wqkv.weight\": params[\"Block_17\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"18.mixer.Wqkv.bias\": params[\"Block_17\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"18.mixer.out_proj.weight\": params[\"Block_17\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"18.mixer.out_proj.bias\": params[\"Block_17\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"18.mlp.fc1.weight\": params[\"Block_17\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"18.mlp.fc1.bias\": params[\"Block_17\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"18.mlp.fc2.weight\": params[\"Block_17\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"18.mlp.fc2.bias\": params[\"Block_17\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"19.ln.weight\": params[\"Block_18\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"19.ln.bias\": params[\"Block_18\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"19.mixer.Wqkv.weight\": params[\"Block_18\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"19.mixer.Wqkv.bias\": params[\"Block_18\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"19.mixer.out_proj.weight\": params[\"Block_18\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"19.mixer.out_proj.bias\": params[\"Block_18\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"19.mlp.fc1.weight\": params[\"Block_18\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"19.mlp.fc1.bias\": params[\"Block_18\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"19.mlp.fc2.weight\": params[\"Block_18\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"19.mlp.fc2.bias\": params[\"Block_18\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"20.ln.weight\": params[\"Block_19\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"20.ln.bias\": params[\"Block_19\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"20.mixer.Wqkv.weight\": params[\"Block_19\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"20.mixer.Wqkv.bias\": params[\"Block_19\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"20.mixer.out_proj.weight\": params[\"Block_19\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"20.mixer.out_proj.bias\": params[\"Block_19\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"20.mlp.fc1.weight\": params[\"Block_19\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"20.mlp.fc1.bias\": params[\"Block_19\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"20.mlp.fc2.weight\": params[\"Block_19\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"20.mlp.fc2.bias\": params[\"Block_19\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"21.ln.weight\": params[\"Block_20\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"21.ln.bias\": params[\"Block_20\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"21.mixer.Wqkv.weight\": params[\"Block_20\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"21.mixer.Wqkv.bias\": params[\"Block_20\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"21.mixer.out_proj.weight\": params[\"Block_20\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"21.mixer.out_proj.bias\": params[\"Block_20\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"21.mlp.fc1.weight\": params[\"Block_20\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"21.mlp.fc1.bias\": params[\"Block_20\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"21.mlp.fc2.weight\": params[\"Block_20\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"21.mlp.fc2.bias\": params[\"Block_20\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"22.ln.weight\": params[\"Block_21\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"22.ln.bias\": params[\"Block_21\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"22.mixer.Wqkv.weight\": params[\"Block_21\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"22.mixer.Wqkv.bias\": params[\"Block_21\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"22.mixer.out_proj.weight\": params[\"Block_21\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"22.mixer.out_proj.bias\": params[\"Block_21\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"22.mlp.fc1.weight\": params[\"Block_21\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"22.mlp.fc1.bias\": params[\"Block_21\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"22.mlp.fc2.weight\": params[\"Block_21\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"22.mlp.fc2.bias\": params[\"Block_21\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"23.ln.weight\": params[\"Block_22\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"23.ln.bias\": params[\"Block_22\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"23.mixer.Wqkv.weight\": params[\"Block_22\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"23.mixer.Wqkv.bias\": params[\"Block_22\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"23.mixer.out_proj.weight\": params[\"Block_22\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"23.mixer.out_proj.bias\": params[\"Block_22\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"23.mlp.fc1.weight\": params[\"Block_22\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"23.mlp.fc1.bias\": params[\"Block_22\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"23.mlp.fc2.weight\": params[\"Block_22\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"23.mlp.fc2.bias\": params[\"Block_22\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"24.ln.weight\": params[\"Block_23\"][\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"24.ln.bias\": params[\"Block_23\"][\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"24.mixer.Wqkv.weight\": params[\"Block_23\"][\"SelfAttention_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"24.mixer.Wqkv.bias\": params[\"Block_23\"][\"SelfAttention_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"24.mixer.out_proj.weight\": params[\"Block_23\"][\"SelfAttention_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"24.mixer.out_proj.bias\": params[\"Block_23\"][\"SelfAttention_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"24.mlp.fc1.weight\": params[\"Block_23\"][\"MLP_0\"][\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"24.mlp.fc1.bias\": params[\"Block_23\"][\"MLP_0\"][\"Dense_0\"][\"bias\"] = jnp_array\n",
    "            case \"24.mlp.fc2.weight\": params[\"Block_23\"][\"MLP_0\"][\"Dense_1\"][\"kernel\"] = jnp_array\n",
    "            case \"24.mlp.fc2.bias\": params[\"Block_23\"][\"MLP_0\"][\"Dense_1\"][\"bias\"] = jnp_array\n",
    "            case \"25.ln.weight\": params[\"LayerNorm_0\"][\"scale\"] = jnp_array\n",
    "            case \"25.ln.bias\": params[\"LayerNorm_0\"][\"bias\"] = jnp_array\n",
    "            case \"25.ln.weight\": params[\"Dense_0\"][\"kernel\"] = jnp_array\n",
    "            case \"25.ln.bias\": params[\"Dense_0\"][\"bias\"] = jnp_array\n",
    "    print(\"model loaded into flax\")\n",
    "    return params\n",
    "\n",
    "def init_train_state(config, batch_size, model_path) -> TrainState:\n",
    "    phi = Phi(config)\n",
    "    # variables = jit(phi.init)(random.PRNGKey(0), jnp.ones((batch_size // jax.local_device_count(), config.n_positions), dtype=jnp.int32))\n",
    "    params = load_model_into_flax(config, model_path)\n",
    "\n",
    "    partition_optimizers = {\"trainable\": adamw(3e-4), \"frozen\": set_to_zero()}\n",
    "    param_partitions = traverse_util.path_aware_map(\n",
    "        lambda path, _: \"trainable\" if (\"Dense_2\" in path or  \"Dense_3\" in path) else \"frozen\", params)\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=phi.apply,\n",
    "        tx=multi_transform(partition_optimizers, param_partitions),\n",
    "        params=params\n",
    "    )\n",
    "    return replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(pmap, axis_name=\"device\")\n",
    "def train_step(state: TrainState, batch: Array):\n",
    "    def loss_fn(params):\n",
    "        _, loss = state.apply_fn({\"params\": params}, batch)\n",
    "        return loss\n",
    "    \n",
    "    grad_fn = value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    grads = pmean(grads, axis_name=\"device\")\n",
    "    loss = pmean(loss, axis_name=\"device\")\n",
    "    return state.apply_gradients(grads=grads), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state: TrainState, n_iter: int, batch_size: int, config: PhiConfig) -> TrainState:\n",
    "    dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_gen\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    n_devices = jax.local_device_count()\n",
    "\n",
    "    loss = 0\n",
    "    tqdm_range = tqdm(range(n_iter))\n",
    "    for i in tqdm_range:\n",
    "        batch = jnp.array(tokenizer([dataset[i * batch_size + j][\"prompt\"] for j in range(batch_size * n_devices)], padding=\"max_length\", max_length=config.n_positions, truncation=True)[\"input_ids\"], dtype=jnp.int32)\n",
    "        batch = jax.tree_map(lambda x: x.reshape((n_devices, -1, *x.shape[1:])), batch)\n",
    "\n",
    "        state, loss = train_step(state, batch)\n",
    "        tqdm_range.set_description(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "    return state\n",
    "        \n",
    "def train(batch_size):\n",
    "    config = PhiConfig()\n",
    "    state = init_train_state(config, batch_size, \"https://huggingface.co/microsoft/phi-1_5/resolve/main/pytorch_model.bin\")\n",
    "    state = train_epoch(state, 1000, batch_size, config)\n",
    "    save_checkpoint(\"/kaggle/working/phi-jax_2\", state, state.step)\n",
    "    \n",
    "train(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.random.nor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
